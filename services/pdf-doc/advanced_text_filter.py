import re

# ê¸°ì¡´ ì„¤ì •
TH_NOISE = 5

# ---------------- ìƒˆë¡œ ì¶”ê°€/ê°•í™”ë˜ëŠ” ìƒìˆ˜ ----------------
NUKE_MODE = True  # Trueë©´ ì•„ë˜ í•µì‹¬ í† í° í¬í•¨ ë¼ì¸ ì¦‰ì‹œ ì œê±°

# ê¸°ì¡´ NUKE_TOKENSì— ì¶”ê°€
NUKE_TOKENS = [
    "ë³€í™˜ ë°©ì‹", "í‘œì¤€ ë³€í™˜", "ë¹ ë¦„", "html í…œí”Œë¦¿ ì—…ë°ì´íŠ¸",
    "ì›¹ ì¸í„°í˜ì´ìŠ¤ ê°œì„ ", "í•´ëë¶ìŠ¤", "2024ë…„ë„", "ë„ì„œëª©ë¡",
    "DIAT", "ITO", "ìˆ˜í—˜ì„œ", "ì¶œê°„ì‚¬", "ê°€ê²©", "ëŒ€ìƒ", "ë¹„ê³ "
]

# ìƒˆë¡œìš´ íŒ¨í„´ ì¶”ê°€ (ê¸°ì¡´ ì½”ë“œ ë’¤ì— ì¶”ê°€)
LONG_REPETITIVE_PATTERN = re.compile(r"(í•´ëë¶ìŠ¤|DIAT|ITO|ìˆ˜í—˜ì„œ|ì¶œê°„ì‚¬).{0,50}(í•´ëë¶ìŠ¤|DIAT|ITO|ìˆ˜í—˜ì„œ|ì¶œê°„ì‚¬)")
TABLE_METADATA_PATTERN = re.compile(r"^(êµì¬ëª…|ì¶œê°„ì‚¬|ê°€ê²©|ëŒ€ìƒ|ë¹„ê³ )\s*[:ï¼š]?\s*")
REPETITIVE_NUMBERS = re.compile(r"\d{1,2}[,-]\d{1,2}[ê¸‰ë‹¨ê³„]")

# ìƒˆë¡œìš´ ê°•í™”ëœ í•„í„°ë§ í•¨ìˆ˜ ì¶”ê°€
def remove_long_repetitive_content(lines: list[str]) -> list[str]:
    """ê¸´ ë°˜ë³µ ì½˜í…ì¸  ì œê±° (í…Œì´ë¸” ë©”íƒ€ë°ì´í„°, ë°˜ë³µ íŒ¨í„´ ë“±)"""
    if not lines:
        return lines
    
    filtered_lines = []
    repetitive_count = 0
    max_repetitive_lines = 10  # ì—°ì† ë°˜ë³µ ë¼ì¸ ìµœëŒ€ í—ˆìš© ìˆ˜
    
    for line in lines:
        line_stripped = line.strip()
        
        # ë¹ˆ ë¼ì¸ì€ í†µê³¼
        if not line_stripped:
            filtered_lines.append(line)
            repetitive_count = 0
            continue
        
        # í…Œì´ë¸” ë©”íƒ€ë°ì´í„° íŒ¨í„´ ê°ì§€
        if TABLE_METADATA_PATTERN.search(line_stripped):
            repetitive_count += 1
            if repetitive_count > max_repetitive_lines:
                continue  # ë„ˆë¬´ ë§ì€ ë°˜ë³µì´ë©´ ìŠ¤í‚µ
        
        # ê¸´ ë°˜ë³µ íŒ¨í„´ ê°ì§€
        elif LONG_REPETITIVE_PATTERN.search(line_stripped):
            repetitive_count += 1
            if repetitive_count > max_repetitive_lines:
                continue  # ë„ˆë¬´ ë§ì€ ë°˜ë³µì´ë©´ ìŠ¤í‚µ
        
        # ë°˜ë³µì ì¸ ìˆ«ì íŒ¨í„´ (ê¸‰ìˆ˜, ë‹¨ê³„ ë“±)
        elif REPETITIVE_NUMBERS.search(line_stripped):
            repetitive_count += 1
            if repetitive_count > max_repetitive_lines:
                continue
        
        # ë¼ì¸ì´ ë„ˆë¬´ ê¸¸ë©´ (200ì ì´ìƒ) ì ì¬ì  ë©”íƒ€ë°ì´í„°ë¡œ ê°„ì£¼
        elif len(line_stripped) > 200:
            # í•˜ì§€ë§Œ í•œê¸€ ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ ì‹¤ì œ ë‚´ìš©ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³´ì¡´
            hangul_ratio_val = hangul_ratio(line_stripped)
            if hangul_ratio_val < 0.3:  # í•œê¸€ ë¹„ìœ¨ì´ 30% ë¯¸ë§Œì´ë©´ ì œê±°
                continue
        
        else:
            repetitive_count = 0  # ì •ìƒ ë¼ì¸ì´ë©´ ì¹´ìš´í„° ë¦¬ì…‹
        
        filtered_lines.append(line)
    
    return filtered_lines

# ê¸°ì¡´ ì½”ë“œì— ì¶”ê°€í•  í•¨ìˆ˜ë“¤

def hangul_ratio(line: str) -> float:
    """í•œê¸€ ë¹„ìœ¨ ê³„ì‚°"""
    if not line.strip():
        return 0.0
    hangul_pattern = re.compile(r"[ê°€-í£]")
    h = len(hangul_pattern.findall(line))
    return h / max(1, len(line))

def early_block_filter(raw_text: str) -> list[str]:
    """ì´ˆê¸° ë¸”ë¡ í•„í„°ë§"""
    lines = [l for l in raw_text.splitlines()]
    # ê³µë°± ì œê±°
    normed = [l.rstrip("\n") for l in lines]
    # ë¶ˆí•„ìš”í•œ ì™„ì „ ê³µë°± ì œê±°
    cleaned = [l for l in normed if l.strip()]
    return cleaned

def classify_lines(lines: list[str], debug=False):
    """ë¼ì¸ ë¶„ë¥˜ ë° ì ìˆ˜ ë¶€ì—¬"""
    classified = []
    for line in lines:
        score = ui_noise_score(line)
        classified.append((line, score))
        if debug:
            print(f"[{score:2d}] {line[:50]}...")
    return classified

def ui_noise_score(line: str) -> int:
    """UI ë…¸ì´ì¦ˆ ì ìˆ˜ ê³„ì‚°"""
    if not line.strip():
        return 0
    
    score = 0
    line_lower = line.lower()
    
    # NUKE í† í° ì²´í¬
    for token in NUKE_TOKENS:
        if token.lower() in line_lower:
            score += 10
    
    # ê°•í•œ UI í‚¤ì›Œë“œ ì²´í¬
    for keyword in STRONG_UI_KEYWORDS:
        if keyword.lower() in line_lower:
            score += 5
    
    # ë°˜ë³µ íŒ¨í„´ ì²´í¬
    if LONG_REPETITIVE_PATTERN.search(line):
        score += 8
    
    if TABLE_METADATA_PATTERN.search(line):
        score += 6
    
    return score

def dynamic_cutoff(noise_classified):
    """ë™ì  ì»·ì˜¤í”„ ì ìš©"""
    return noise_classified

def second_pass_nuke(kept_lines):
    """2ì°¨ ì œê±° - UI í‚¤ì›Œë“œ ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ í•´ë‹¹ ë¼ì¸ë“¤ ì‚­ì œ"""
    if not kept_lines:
        return kept_lines
    
    norm_lines = [line.lower().replace(" ", "") for line in kept_lines]
    ui_hits = sum(1 for n in norm_lines if any(k in n for k in NOISE_TRIGGER_KEYWORDS))
    
    if ui_hits / len(kept_lines) >= 0.30:
        # 30% ì´ìƒì´ë©´ í•´ë‹¹ í‚¤ì›Œë“œ í¬í•¨ ë¼ì¸ ì‚­ì œ
        filtered = []
        for l, n in zip(kept_lines, norm_lines):
            if any(k in n for k in NOISE_TRIGGER_KEYWORDS):
                continue
            filtered.append(l)
        return filtered if filtered else kept_lines
    
    return kept_lines

def recover_if_too_few(original, filtered):
    """í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê³µë¬¸ì„œ í‚¤ì›Œë“œë¡œ ë³µêµ¬ ì‹œë„"""
    PUBLIC_DOC_KEYWORDS = [
        "ìˆ˜ì‹ ", "ì œëª©", "ë¶™ì„", "ë‹´ë‹¹", "ì—°ë½ì²˜", "íšŒì‹ ", "í˜‘ì¡°", "ì•ˆë‚´",
        "ì‹ ì²­", "ìš”ì²­", "ê³µì§€", "ë°°í¬", "ê²€í† ", "íšŒì˜", "ê³µë¬¸"
    ]
    
    if len(filtered) == 0 or len(filtered) <= 3:
        recover = [l for l in original if any(k in l for k in PUBLIC_DOC_KEYWORDS)]
        if recover:
            return recover
        # ë§ˆì§€ë§‰ ë³´í˜¸: ì›ë³¸ ìƒìœ„ 15ì¤„
        return original[:15]
    return filtered

def final_compact(lines):
    """ìµœì¢… ì••ì¶• - ì¤‘ë³µ ì œê±° & ê³µë°± ë‹¤ë“¬ê¸°"""
    seen = set()
    out = []
    for l in lines:
        # ì •ê·œí™”ëœ í‚¤ë¡œ ì¤‘ë³µ ì²´í¬
        key = "".join(l.split()).lower()
        if key in seen or len(key) < 3:  # ë„ˆë¬´ ì§§ì€ ë¼ì¸ë„ ì œê±°
            continue
        seen.add(key)
        out.append(l.strip())
    return out

# ìƒˆë¡œìš´ ì¤‘ë³µ ì œê±° í•¨ìˆ˜ ì¶”ê°€
def remove_duplicate_content(lines: list[str]) -> list[str]:
    """ì¤‘ë³µ ì½˜í…ì¸  ì œê±° (ì—°ì†ëœ ì¤‘ë³µ ë¸”ë¡ ê°ì§€)"""
    if not lines:
        return lines
    
    filtered_lines = []
    seen_blocks = set()
    current_block = []
    
    for line in lines:
        line_stripped = line.strip()
        
        if not line_stripped:
            # ë¹ˆ ë¼ì¸ì´ë©´ í˜„ì¬ ë¸”ë¡ ì²˜ë¦¬
            if current_block:
                block_key = "\n".join(current_block).lower()
                if block_key not in seen_blocks:
                    seen_blocks.add(block_key)
                    filtered_lines.extend(current_block)
                    filtered_lines.append(line)  # ë¹ˆ ë¼ì¸ë„ ì¶”ê°€
                current_block = []
            continue
        
        current_block.append(line)
        
        # ë¸”ë¡ì´ 5ì¤„ ì´ìƒì´ë©´ ì¤‘ë³µ ì²´í¬
        if len(current_block) >= 5:
            block_key = "\n".join(current_block).lower()
            if block_key not in seen_blocks:
                seen_blocks.add(block_key)
                filtered_lines.extend(current_block)
            current_block = []
    
    # ë§ˆì§€ë§‰ ë¸”ë¡ ì²˜ë¦¬
    if current_block:
        block_key = "\n".join(current_block).lower()
        if block_key not in seen_blocks:
            filtered_lines.extend(current_block)
    
    return filtered_lines

# filter_text_blocks í•¨ìˆ˜ ìˆ˜ì •
def filter_text_blocks(raw_text: str, debug=False) -> str:
    """í†µí•©: ê¸°ì¡´ filter_text_blocks ì¬ì •ì˜ (ì ìˆ˜ ê¸°ë°˜ í•„í„° í˜¸ì¶œ ì „)"""
    # 0) Early block nuke
    early_lines = early_block_filter(raw_text)
    
    # 0.3) ì¤‘ë³µ ì½˜í…ì¸  ì œê±° (ìƒˆë¡œ ì¶”ê°€)
    early_lines = remove_duplicate_content(early_lines)
    
    # 0.5) ê¸´ ë°˜ë³µ ì½˜í…ì¸  ì œê±°
    early_lines = remove_long_repetitive_content(early_lines)
    
    # 1) ì ìˆ˜ ê¸°ë°˜
    classified = classify_lines(early_lines, debug=debug)
    after_cut = dynamic_cutoff(classified)
    kept = [l for (l, sc) in after_cut if sc < TH_NOISE]
    
    kept2 = second_pass_nuke(kept)
    recovered = recover_if_too_few(early_lines, kept2)
    final_lines = final_compact(recovered)
    
    return "\n".join(final_lines)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    test_text = """
    ë³€í™˜ ë°©ì‹:
    í‘œì¤€ ë³€í™˜ (ë¹ ë¦„)
    ### HTML í…œí”Œë¦¿ ì—…ë°ì´íŠ¸:
    ```html
    <div>íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”</div>
    ```
    ## ğŸ¯ 4. ì›¹ ì¸í„°í˜ì´ìŠ¤ ê°œì„ :
    ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤.
    ì´ê²ƒì€ ë³´ì¡´ë˜ì–´ì•¼ í•  í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
    ê³µì§€ì‚¬í•­: ì¤‘ìš”í•œ ë‚´ìš©
    ë‹´ë‹¹ì: í™ê¸¸ë™
    """
    
    print("=== ì›ë³¸ í…ìŠ¤íŠ¸ ===")
    print(test_text)
    
    print("\n=== ê°•í™”ëœ í•„í„°ë§ ê²°ê³¼ ===")
    filtered = filter_text_blocks(test_text, debug=True)
    print(filtered)

# STRONG_UI_KEYWORDSì— ì¶”ê°€
STRONG_UI_KEYWORDS = [
    "ë³€í™˜ë°©ì‹", "ë³€í™˜ë°©ì‹:", "htmlí…œí”Œë¦¿ì—…ë°ì´íŠ¸", "íŒŒì¼ì„ì„ íƒ", "íŒŒì¼ì„ íƒ",
    "í´ë¦­í•˜ì—¬íŒŒì¼ì„ íƒ", "pdfpptx", "pptx-pdf", "p p t x", "ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤",
    "ocrì§€ì›", "ì´ë¯¸ì§€í’ˆì§ˆ", "ìŠ¬ë¼ì´ë“œë‹¹í…ìŠ¤íŠ¸", "dpi", "pptx", "pdf",
    "ì—…ë¡œë“œ", "ë³€í™˜ì™„ë£Œ", "ë³€í™˜í•˜ê¸°",
    # ìƒˆë¡œ ì¶”ê°€ëœ í‚¤ì›Œë“œë“¤
    "í•´ëë¶ìŠ¤", "2024ë…„ë„", "ë„ì„œëª©ë¡", "ìˆ˜í—˜ì„œ", "ì¶œê°„ì‚¬", "êµì¬ëª…",
    "DIAT", "ITO", "ê¸‰ìˆ˜", "ë‹¨ê³„", "ê°€ê²©", "ëŒ€ìƒ", "ë¹„ê³ "
]

# NOISE_TRIGGER_KEYWORDSì— ì¶”ê°€
NOISE_TRIGGER_KEYWORDS = [
    "ë³€í™˜", "íŒŒì¼", "pptx", "pdf", "í…œí”Œë¦¿", "ì—…ë°ì´íŠ¸",
    # ìƒˆë¡œ ì¶”ê°€
    "í•´ëë¶ìŠ¤", "ìˆ˜í—˜ì„œ", "ì¶œê°„ì‚¬", "êµì¬ëª…", "ë„ì„œëª©ë¡"
]